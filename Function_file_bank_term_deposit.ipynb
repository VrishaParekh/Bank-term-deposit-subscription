{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Author__ - Vrisha Parekh\n",
    "\n",
    "__Email__ - parekh.vrisha@gmail.com\n",
    "\n",
    "\n",
    "__LinkedIn__ - https://bit.ly/VrishaParekh_LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from scipy.stats import norm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import classification_report,confusion_matrix,recall_score,precision_score,accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "def load_csv(file):\n",
    "    #load csv file\n",
    "    data=pd.read_csv(file,sep=';')\n",
    "    df=pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Removing the duplicates\n",
    "def clean_df(df):\n",
    "    clean_data = df.drop_duplicates()\n",
    "    return clean_data\n",
    "\n",
    "\n",
    "#Removing extreme values\n",
    "def remove_extreme_age(df):\n",
    "    #Removing age values less than 18 yrs as they would not be eligible for term deposit.\n",
    "    df=df[df['age']>18]\n",
    "    \n",
    "    #Dropping the extreme campaign values\n",
    "    df=df[df['campaign']<56]\n",
    "    return df\n",
    "\n",
    "\n",
    "#Transforming feature\n",
    "def transform_features(df):\n",
    "    #Age is right skewed so log transforming age\n",
    "    df['age']=np.log(df['age']+1)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Dropping column\n",
    "def drop_columns(df,col_name):\n",
    "    #dropping duration as it highly influences our response variable\n",
    "    df.drop(col_name,inplace=True,axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Replacing yes and no in response variable with 0 and 1\n",
    "def replace_response_variable(df):\n",
    "    df['y'].replace(['yes','no'],[1,0],inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Converting categorical features to dummy variables\n",
    "def get_dummies_func(df,cat_list):\n",
    "    #Creating dummies for categorical data \n",
    "    return pd.concat([pd.get_dummies(data=df,columns=cat_list)],axis=1)\n",
    "\n",
    "\n",
    "#Spliting the data into Train and test sets\n",
    "def split(col_name):\n",
    "    \n",
    "    X= df[[i for i in list(df.columns) if i!=col_name]].values\n",
    "    y=df[col_name]\n",
    "    \n",
    "    X_train,X_test,y_train,y_test=sklearn_train_test_split(X,y,test_size=0.25,random_state=42)\n",
    "    \n",
    "    #Scaling the splitted data\n",
    "    scaler=preprocessing.StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train=scaler.transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "#Observing the baseline performance of three different models, Logistic Regression, Decision tree and Random Forest\n",
    "\n",
    "def benchmark_models(model):\n",
    "\n",
    "    initial_models=model.fit(X_train,y_train)\n",
    "    y_pred=initial_models.predict(X_test)\n",
    "    reportss=classification_report(y_test,y_pred)\n",
    "    return reportss\n",
    "\n",
    "\n",
    "#Using a mix of two sampling techniques(Smote-Oversampling,Tomek-Undersampling)\n",
    "def sampling():\n",
    "    \n",
    "    Xdash= df[[i for i in list(df.columns) if i!='y']]\n",
    "    columns=Xdash.columns\n",
    "\n",
    "    smt=SMOTETomek(sampling_strategy= 'auto')\n",
    "    X_smt,y_smt=smt.fit_sample(X_train,y_train)\n",
    "    \n",
    "    #Creating dataframes\n",
    "    X_smt_df=pd.DataFrame(data=X_smt,columns=columns)\n",
    "    y_smt_df=pd.DataFrame(data=y_smt,columns=['y'])\n",
    "    \n",
    "    #Checking the number of samples for both the classes\n",
    "    print('Number of NO subscription in oversampled data',len(y_smt_df[y_smt_df['y']==0]))\n",
    "    print('Number of YES subscription in oversampled data',len(y_smt_df[y_smt_df['y']==1]))\n",
    "    return X_smt,y_smt,X_smt_df\n",
    "\n",
    "\n",
    "#Defining summary metric\n",
    "def summary_metrics(y_pred):\n",
    "    \n",
    "    conf_matrix= confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print('confusion matrix',conf_matrix)\n",
    "    print('Accuracy',accuracy_score(y_test,y_pred))\n",
    "    print('Precision',precision_score(y_test,y_pred))\n",
    "    print('Recall',recall_score(y_test,y_pred))\n",
    "    \n",
    "\n",
    "\n",
    "#Applying rfe and cross valuation to our sampled data\n",
    "\n",
    "def elimination_crossval(model):\n",
    "    \n",
    "    #Initiating the RFE instance\n",
    "    rfe=RFE(estimator=RandomForestClassifier(),n_features_to_select=10)\n",
    "    \n",
    "    #Fitting the rfe\n",
    "    X_rfe=rfe.fit_transform(X_smt,y_smt)\n",
    "    \n",
    "    #Transforming X_test\n",
    "    X_rfe_test=rfe.transform(X_test)\n",
    "    \n",
    "    model=model\n",
    "    \n",
    "    #Creating pipeling to avoid data leakage\n",
    "    pipeline=Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    \n",
    "    cv=RepeatedStratifiedKFold(n_splits=10,n_repeats=3,random_state=1)\n",
    "    \n",
    "    scores =cross_val_score(pipeline,X_rfe, y_smt, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    \n",
    "    print('Accuracy for model with cross val: %.3f (%.3f)' % (mean(scores)*100, std(scores)*100))\n",
    "    \n",
    "    #Fitting the pipeline\n",
    "    fitted_model=pipeline.fit(X_rfe,y_smt)\n",
    "    \n",
    "    y_pred=fitted_model.predict(X_rfe_test)\n",
    "    \n",
    "    #Printing the classification report\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    \n",
    "    summary_metrics(y_pred)\n",
    "    \n",
    "    \n",
    "#Getting the important features\n",
    "    \n",
    "def important_features(estimator,n_features_to_select):\n",
    "    \n",
    "    rfe=RFE(estimator=estimator, n_features_to_select=n_features_to_select)\n",
    "    X_rfe=rfe.fit_transform(X_smt,y_smt)\n",
    "\n",
    "\n",
    "    columns = X_smt_df.columns\n",
    "    val = pd.Series(rfe.support_,index = columns)\n",
    "    features_chosen_rfe = val[val==True].index \n",
    "    print(features_chosen_rfe)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NO subscription in oversampled data 27117\n",
      "Number of YES subscription in oversampled data 27117\n",
      "Accuracy for model with cross val: 72.445 (0.578)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.74      0.83      9154\n",
      "           1       0.25      0.70      0.37      1132\n",
      "\n",
      "    accuracy                           0.74     10286\n",
      "   macro avg       0.60      0.72      0.60     10286\n",
      "weighted avg       0.88      0.74      0.78     10286\n",
      "\n",
      "confusion matrix [[6781 2373]\n",
      " [ 338  794]]\n",
      "Accuracy 0.7364378767256465\n",
      "Precision 0.25071045153141774\n",
      "Recall 0.7014134275618374\n",
      "Accuracy for model with cross val: 92.028 (0.265)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93      9154\n",
      "           1       0.41      0.37      0.39      1132\n",
      "\n",
      "    accuracy                           0.87     10286\n",
      "   macro avg       0.67      0.65      0.66     10286\n",
      "weighted avg       0.87      0.87      0.87     10286\n",
      "\n",
      "confusion matrix [[8557  597]\n",
      " [ 712  420]]\n",
      "Accuracy 0.8727396461209411\n",
      "Precision 0.41297935103244837\n",
      "Recall 0.3710247349823322\n",
      "Index(['age', 'campaign', 'euribor3m', 'nr.employed'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Loading data\n",
    "df=load_csv('bank-additional-full.csv')\n",
    "\n",
    "#Removing the duplicates\n",
    "df=clean_df(df)\n",
    "\n",
    "#Removing extreme values\n",
    "df=remove_extreme_age(df)\n",
    "\n",
    "#Transforming feature\n",
    "df=transform_features(df)\n",
    "\n",
    "#Dropping column\n",
    "df=drop_columns(df,'duration')\n",
    "\n",
    "#Replacing yes and no in response variable with 0 and 1\n",
    "df=replace_response_variable(df)\n",
    "\n",
    "#Converting categorical features to dummy variables\n",
    "cat_list= ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact','month', 'day_of_week', 'poutcome']\n",
    "\n",
    "df=get_dummies_func(df,cat_list)\n",
    "\n",
    "#Spliting the data into Train and test sets\n",
    "X_train,X_test,y_train,y_test=split('y')\n",
    "\n",
    "\n",
    "#Observing the baseline performance of three different models, Logistic Regression, Decision tree and Random Forest\n",
    "reports_logistic=benchmark_models(LogisticRegression(max_iter=7600))\n",
    "reports_Decisiontree=benchmark_models(DecisionTreeClassifier())\n",
    "reports_RandomForest=benchmark_models(RandomForestClassifier())\n",
    "\n",
    "\n",
    "#Using a mix of two sampling techniques(Smote-Oversampling,Tomek-Undersampling)\n",
    "X_smt,y_smt,X_smt_df=sampling()\n",
    "\n",
    "\n",
    "#Applying rfe and cross valuation to our sampled data\n",
    "elimination_crossval(LogisticRegression(max_iter=7600))\n",
    "elimination_crossval(RandomForestClassifier())\n",
    "\n",
    "#Getting the important features\n",
    "important_features(RandomForestClassifier(),4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
